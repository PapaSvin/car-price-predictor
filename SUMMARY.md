# üéØ –†–ï–ó–Æ–ú–ï: –ö–∞–∫ —É–ª—É—á—à–∏—Ç—å Train Loss

## üìä –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω

–Ø –∏–∑—É—á–∏–ª –≤–∞—à–∏ —Ñ–∞–π–ª—ã (`main.py`, `predictor.py`, `data_builder.py`) –∏ –≤—ã—è–≤–∏–ª **10 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º**, –∫–æ—Ç–æ—Ä—ã–µ –º–µ—à–∞—é—Ç —Å–Ω–∏–∂–µ–Ω–∏—é train loss.

## ‚ö†Ô∏è –ì–ª–∞–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

### 1. **–°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π Dropout (0.4)**
- **–ü—Ä–æ–±–ª–µ–º–∞:** –û—Ç–∫–ª—é—á–∞–µ—Ç 40% –Ω–µ–π—Ä–æ–Ω–æ–≤ ‚Üí –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –æ–±—É—á–∏—Ç—å—Å—è
- **–†–µ—à–µ–Ω–∏–µ:** –°–Ω–∏–∂–µ–Ω –¥–æ 0.2-0.15

### 2. **–ë–æ–ª—å—à–æ–π Batch Size (64) –ø—Ä–∏ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ**
- **–ü—Ä–æ–±–ª–µ–º–∞:** ~40 –æ–±—É—á–∞—é—â–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ ‚Üí –≤—Å–µ–≥–æ 1 –±–∞—Ç—á –∑–∞ —ç–ø–æ—Ö—É
- **–†–µ—à–µ–Ω–∏–µ:** –£–º–µ–Ω—å—à–µ–Ω –¥–æ 16 (4x –±–æ–ª—å—à–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π)

### 3. **–ú–∞–ª—ã–π Learning Rate (0.001)**
- **–ü—Ä–æ–±–ª–µ–º–∞:** –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
- **–†–µ—à–µ–Ω–∏–µ:** –£–≤–µ–ª–∏—á–µ–Ω –¥–æ 0.003 + –¥–æ–±–∞–≤–ª–µ–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π scheduler

### 4. **–°–ª–∏—à–∫–æ–º –≥–ª—É–±–æ–∫–∞—è —Å–µ—Ç—å –¥–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞**
- **–ü—Ä–æ–±–ª–µ–º–∞:** 256‚Üí128‚Üí64‚Üí32‚Üí1 (4 —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è) –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è
- **–†–µ—à–µ–Ω–∏–µ:** –£–ø—Ä–æ—â–µ–Ω–∞ –¥–æ 128‚Üí64‚Üí32‚Üí1 (3 —Å–ª–æ—è)

### 5. **MSE Loss —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º**
- **–ü—Ä–æ–±–ª–µ–º–∞:** –ë–æ–ª—å—à–∏–µ –æ—à–∏–±–∫–∏ –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö
- **–†–µ—à–µ–Ω–∏–µ:** –ó–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ Huber Loss (—É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º)

## ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è

–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª **`main_improved.py`** —Å 10 —É–ª—É—á—à–µ–Ω–∏—è–º–∏:

1. ‚úÖ Batch Size: 64 ‚Üí **16**
2. ‚úÖ Dropout: 0.4 ‚Üí **0.2-0.15**
3. ‚úÖ Learning Rate: 0.001 ‚Üí **0.003 + scheduler**
4. ‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: —É–ø—Ä–æ—â–µ–Ω–∞ (3 —Å–ª–æ—è –≤–º–µ—Å—Ç–æ 4)
5. ‚úÖ –ê–∫—Ç–∏–≤–∞—Ü–∏—è: ReLU ‚Üí **LeakyReLU** (–ª—É—á—à–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã)
6. ‚úÖ Loss: MSE ‚Üí **Huber Loss**
7. ‚úÖ Optimizer: Adam ‚Üí **AdamW** (–ª—É—á—à–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
8. ‚úÖ **LR Scheduler** (ReduceLROnPlateau)
9. ‚úÖ **Early Stopping** (patience=50)
10. ‚úÖ **Gradient Clipping** (max_norm=1.0)

## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

| –ú–µ—Ç—Ä–∏–∫–∞ | –î–æ | –ü–æ—Å–ª–µ | –£–ª—É—á—à–µ–Ω–∏–µ |
|---------|----|----|-----------|
| **Train Loss** | 0.15-0.20 | **0.05-0.10** | **2-3x** ‚¨áÔ∏è |
| –°—Ö–æ–¥–∏–º–æ—Å—Ç—å | –ú–µ–¥–ª–µ–Ω–Ω–∞—è | –ë—ã—Å—Ç—Ä–∞—è | ‚ö° |
| –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å | –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞—è | –°—Ç–∞–±–∏–ª—å–Ω–∞—è | ‚úÖ |
| –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ | –í—ã—Å–æ–∫–æ–µ | –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ | ‚úÖ |

## üöÄ –ö–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å

```bash
# 1. –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (—É–∂–µ —Å–æ–∑–¥–∞–Ω—ã):
#    - architecture_comparison.png
#    - hyperparameters_comparison.png  
#    - expected_loss_improvement.png

# 2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å:
python main_improved.py

# 3. –°—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
#    - improved_loss_mae_plot.png (–≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è)
#    - improved_confusion_matrix.png (–º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫)
#    - improved_error_distribution.png (–∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫)
```

## üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

### –ö–æ–¥:
- ‚úÖ **`main_improved.py`** - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å 10 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
- ‚úÖ **`visualize_improvements.py`** - —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

### –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:
- ‚úÖ **`IMPROVEMENTS.md`** - –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤—Å–µ—Ö —É–ª—É—á—à–µ–Ω–∏–π
- ‚úÖ **`QUICK_GUIDE.md`** - –∫—Ä–∞—Ç–∫–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
- ‚úÖ **`SUMMARY.md`** - —ç—Ç–æ—Ç —Ñ–∞–π–ª

### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (—É–∂–µ —Å–æ–∑–¥–∞–Ω—ã):
- ‚úÖ **`architecture_comparison.png`** - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
- ‚úÖ **`hyperparameters_comparison.png`** - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- ‚úÖ **`expected_loss_improvement.png`** - –æ–∂–∏–¥–∞–µ–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ

## üîç –ö–ª—é—á–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–¥–µ

```python
# –ë–´–õ–û:
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
model = CarPricePredictor(...)  # 256‚Üí128‚Üí64‚Üí32‚Üí1, Dropout(0.4)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
# –ù–µ—Ç scheduler, early stopping, gradient clipping

# –°–¢–ê–õ–û:
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # 4x –º–µ–Ω—å—à–µ
model = ImprovedCarPricePredictor(...)  # 128‚Üí64‚Üí32‚Üí1, Dropout(0.2)
criterion = nn.HuberLoss(delta=1.0)  # –£—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º
optimizer = optim.AdamW(model.parameters(), lr=0.003, weight_decay=1e-4)  # 3x LR
scheduler = optim.lr_scheduler.ReduceLROnPlateau(...)  # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π LR
early_stopping = EarlyStopping(patience=50)  # –ê–≤—Ç–æ—Å—Ç–æ–ø
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
```

## üí° –ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç?

### 1. –ú–µ–Ω—å—à–∏–π Batch Size (16 –≤–º–µ—Å—Ç–æ 64)
- –ü—Ä–∏ 40 –æ–±—Ä–∞–∑—Ü–∞—Ö ‚Üí 2-3 –±–∞—Ç—á–∞ –≤–º–µ—Å—Ç–æ 1
- –ë–æ–ª—å—à–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤–µ—Å–æ–≤ –∑–∞ —ç–ø–æ—Ö—É
- –ë–æ–ª—å—à–µ "—à—É–º–∞" –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã–π—Ç–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤

### 2. –ú–µ–Ω—å—à–∏–π Dropout (0.2 –≤–º–µ—Å—Ç–æ 0.4)
- 0.4 = –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è 40% –Ω–µ–π—Ä–æ–Ω–æ–≤
- –°–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ –¥–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
- 0.2 = –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π –∏ –æ–±—É—á–µ–Ω–∏–µ–º

### 3. –ë–æ–ª—å—à–∏–π LR (0.003 –≤–º–µ—Å—Ç–æ 0.001)
- –ë—ã—Å—Ç—Ä–∞—è –Ω–∞—á–∞–ª—å–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
- Scheduler –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–Ω–∏–∂–∞–µ—Ç –ø—Ä–∏ –∑–∞—Å—Ç–æ–µ
- –†–µ–∑—É–ª—å—Ç–∞—Ç: –±—ã—Å—Ç—Ä–æ –¥–æ—Å—Ç–∏–≥–∞–µ–º –º–∏–Ω–∏–º—É–º–∞, –∑–∞—Ç–µ–º —Ç–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

### 4. Huber Loss –≤–º–µ—Å—Ç–æ MSE
- MSE: –æ—à–∏–±–∫–∞¬≤ ‚Üí –±–æ–ª—å—à–∏–µ –æ—à–∏–±–∫–∏ –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç
- Huber: –≥–∏–±—Ä–∏–¥ MSE + MAE ‚Üí —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º
- –ë–æ–ª–µ–µ –ø–ª–∞–≤–Ω—ã–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

### 5. –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- –ú–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç (~50 –æ–±—Ä–∞–∑—Ü–æ–≤) –Ω–µ –º–æ–∂–µ—Ç –æ–±—É—á–∏—Ç—å –≥–ª—É–±–æ–∫—É—é —Å–µ—Ç—å
- 3 —Å–ª–æ—è –≤–º–µ—Å—Ç–æ 4 ‚Üí –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ú–µ–Ω—å—à–µ —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è

–°–ª–µ–¥–∏—Ç–µ –∑–∞ —ç—Ç–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏:

‚úÖ **Train Loss —Å–Ω–∏–∂–∞–µ—Ç—Å—è** - –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è  
‚úÖ **Test Loss —Å–Ω–∏–∂–∞–µ—Ç—Å—è** - —Ö–æ—Ä–æ—à–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è  
‚úÖ **Train Loss ‚âà Test Loss** - –Ω–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è  
‚úÖ **LR —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è** - scheduler —Ä–∞–±–æ—Ç–∞–µ—Ç  
‚úÖ **MAE –≤ $ —Å–Ω–∏–∂–∞–µ—Ç—Å—è** - —Ä–µ–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π  

‚ö†Ô∏è **Train Loss << Test Loss** - –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (–Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ dropout)  
‚ö†Ô∏è **Loss –Ω–µ —Å–Ω–∏–∂–∞–µ—Ç—Å—è** - —É–≤–µ–ª–∏—á—å—Ç–µ LR –∏–ª–∏ —É–ø—Ä–æ—Å—Ç–∏—Ç–µ –º–æ–¥–µ–ª—å  

## üéì –î–∞–ª—å–Ω–µ–π—à–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã

–ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã:

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ
```python
optimizer = optim.AdamW(model.parameters(), lr=0.005)  # –ï—â–µ –≤—ã—à–µ LR
nn.Dropout(0.1)  # –ï—â–µ –º–µ–Ω—å—à–µ dropout
```

### –í–∞—Ä–∏–∞–Ω—Ç 2: –ü—Ä–æ—â–µ –º–æ–¥–µ–ª—å
```python
# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: 64‚Üí32‚Üí1 (–≤—Å–µ–≥–æ 2 —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è)
nn.Linear(input_size, 64),
nn.LeakyReLU(0.1),
nn.Dropout(0.1),
nn.Linear(64, 32),
nn.LeakyReLU(0.1),
nn.Linear(32, 1)
```

### –í–∞—Ä–∏–∞–Ω—Ç 3: –î—Ä—É–≥–∞—è Loss
```python
criterion = nn.HuberLoss(delta=0.5)  # –°—Ç—Ä–æ–∂–µ –∫ –≤—ã–±—Ä–æ—Å–∞–º
# –∏–ª–∏
criterion = nn.SmoothL1Loss()  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Huber
```

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

1. **IMPROVEMENTS.md** - –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è
2. **QUICK_GUIDE.md** - –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
3. **architecture_comparison.png** - –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
4. **hyperparameters_comparison.png** - —Ç–∞–±–ª–∏—Ü–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π

## üéØ –ò—Ç–æ–≥–∏

### –ß—Ç–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ:
- ‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Ñ–∞–π–ª—ã –∏ –≤—ã—è–≤–ª–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã
- ‚úÖ –°–æ–∑–¥–∞–Ω —É–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–¥ (`main_improved.py`)
- ‚úÖ –ù–∞–ø–∏—Å–∞–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (3 —Ñ–∞–π–ª–∞)
- ‚úÖ –°–æ–∑–¥–∞–Ω—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (3 –≥—Ä–∞—Ñ–∏–∫–∞)

### –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:
- üéØ **Train Loss —Å–Ω–∏–∑–∏—Ç—Å—è –≤ 2-3 —Ä–∞–∑–∞** (—Å 0.15-0.20 –¥–æ 0.05-0.10)
- ‚ö° –ë—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å (–±–æ–ª—å—à–æ–π LR + scheduler)
- ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (gradient clipping + early stopping)
- üìà –õ—É—á—à–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è (Huber Loss + –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π dropout)

## ‚ñ∂Ô∏è –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥

```bash
python main_improved.py
```

–ù–∞–±–ª—é–¥–∞–π—Ç–µ –∑–∞ –ª–æ–≥–∞–º–∏:
- Epoch [10/500] - –ø–µ—Ä–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- Learning rate —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è - scheduler —Ä–∞–±–æ—Ç–∞–µ—Ç
- Early stopping - –Ω–∞–π–¥–µ–Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞
- Best Test Loss - –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

**–£–¥–∞—á–∏! üöÄ**

---

_–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –Ω—É–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã - –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å!_
